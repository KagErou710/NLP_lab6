{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n",
    "\n",
    "In this lecture, we will explore the architecture of DistilBERT, its key components, and how it can be utilized for various natural language processing tasks. Additionally, we'll discuss its advantages, limitations, and provide hands-on examples to showcase its effectiveness.\n",
    "\n",
    "Reference : [The Theory](https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a) | [Code](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "# # import os\n",
    "# os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.16.1', '4.38.2', '2.1.2+cpu')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets --upgrade\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "datasets.__version__, transformers.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading our MNLI part of the GLUE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "###1. Load Dataset\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "task_name = \"mnli\"\n",
    "raw_datasets = datasets.load_dataset(\"glue\", task_name)\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = raw_datasets.split\n",
    "# raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'neutral': 1, 'contradiction': 2}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = raw_datasets['train'].features['label'].names\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'neutral', 2: 'contradiction'}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {i: v for v, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "num_labels = np.unique(raw_datasets['train']['label']).size\n",
    "num_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"figures/BERT_embed.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "teacher_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(transformers.is_torch_available())\n",
    "from transformers import BertTokenizer, BertModel\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels = num_labels,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")\n",
    "\n",
    "teacher_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, max_length=128, truncation=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['premise', 'hypothesis']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(task_to_keys[task_name])\n",
    "column_dataset = [item for item in task_to_keys[task_name] if item is not None]\n",
    "column_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove column : 'premise', 'hypothesis', 'idx'\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(column_dataset + [\"idx\"])\n",
    "#rename column : 'labels'\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 17158,  2135,  6949,  8301, 25057,  2038,  2048,  3937,  9646,\n",
       "         1011,  4031,  1998, 10505,  1012,   102,  4031,  1998, 10505,  2024,\n",
       "         2054,  2191,  6949,  8301, 25057,  2147,  1012,   102])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "#Data collator that will dynamically pad the inputs received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation_mismatched\"].shuffle(seed=1150).select(range(100))\n",
    "small_test_dataset = tokenized_datasets[\"test_mismatched\"].shuffle(seed=1150).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    small_train_dataset, shuffle=True, batch_size=32, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(\n",
    "    small_test_dataset, batch_size=32, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(\n",
    "    small_eval_dataset, batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.Size([32, 79]), torch.Size([32, 79]))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "    \n",
    "batch['labels'].shape, batch['input_ids'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model and losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Teacher Model & Student Model\n",
    "\n",
    "####  Architecture \n",
    "In the present work, the student - DistilBERT - has the same general architecture as BERT. \n",
    "- The `token-type embeddings` and the `pooler` are removed while `the number of layers` is reduced by a factor of 2. \n",
    "- Most of the operations used in the Transformer architecture `linear layer` and `layer normalisation` are highly optimized in modern linear algebra frameworks.\n",
    "- our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. \n",
    "- Thus we focus on reducing the number of layers.\n",
    "\n",
    "#### Initialize Student Model\n",
    "- To initialize a new model from an existing one, we need to access the weights of the old model (the teacher). \n",
    "- In order to get the weights, we first have to know how to access them. We’ll use BERT as our teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"entailment\",\n",
       "    \"1\": \"neutral\",\n",
       "    \"2\": \"contradiction\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"contradiction\": 2,\n",
       "    \"entailment\": 0,\n",
       "    \"neutral\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "- The student model has the same configuration, except the number of layers is reduced by a factor of 2\n",
    "- The student layers are initilized by copying one out of two layers of the teacher, starting with layer 0.\n",
    "- The head of the teacher is also copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertConfig\n",
    "# Get teacher configuration as a dictionnary\n",
    "configuration = teacher_model.config.to_dict()\n",
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half the number of hidden layer\n",
    "configuration['num_hidden_layers'] //= 2\n",
    "# Convert the dictionnary to the student configuration\n",
    "configuration = BertConfig.from_dict(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create uninitialized student model\n",
    "model = type(teacher_model)(configuration)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursively copies the weights of the (teacher) to the (student).\n",
    "- This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "- The only part that's not fully copied is the encoder, of which only half is copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel\n",
    "from torch.nn import Module\n",
    "\n",
    "def distill_bert_weights_topK(\n",
    "    teacher : Module,\n",
    "    student : Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model or a BERTFor..., unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights_topK(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]#12 layers\n",
    "        student_encoding_layers = [layer for layer in next(student.children())] #6 layers\n",
    "        # print(len(teacher_encoding_layers))\n",
    "        # print(len(student_encoding_layers))\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            # print(i)\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[i].state_dict())\n",
    "            # student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict()) #even\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_bert_weights_bottomK(\n",
    "    teacher : Module,\n",
    "    student : Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model or a BERTFor..., unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights_bottomK(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]#12 layers\n",
    "        student_encoding_layers = [layer for layer in next(student.children())] #6 layers\n",
    "        # print(len(teacher_encoding_layers))\n",
    "        # print(len(student_encoding_layers))\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            # print(i)\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[i+6].state_dict())\n",
    "            # student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict()) #even\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_bert_weights_odd(\n",
    "    teacher : Module,\n",
    "    student : Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model or a BERTFor..., unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights_odd(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]#12 layers\n",
    "        student_encoding_layers = [layer for layer in next(student.children())] #6 layers\n",
    "        # print(len(teacher_encoding_layers))\n",
    "        # print(len(student_encoding_layers))\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            # print(i)\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i + 1].state_dict())\n",
    "\n",
    "        # print('instance')\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())\n",
    "        # print('no instance')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_bert_weights_even(\n",
    "    teacher : Module,\n",
    "    student : Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a BertFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire BERT model or a BERTFor..., unpack and iterate\n",
    "    if isinstance(teacher, BertModel) or type(teacher).__name__.startswith('BertFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_bert_weights_even(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, BertEncoder):\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())]#12 layers\n",
    "        student_encoding_layers = [layer for layer in next(student.children())] #6 layers\n",
    "        # print(len(teacher_encoding_layers))\n",
    "        # print(len(student_encoding_layers))\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            # print(i)\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict()) #even\n",
    "        # print('instance')    \n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())\n",
    "        # print('no instance')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = distill_bert_weights_topK(teacher=teacher_model, student=model)\n",
    "bottomK = distill_bert_weights_bottomK(teacher=teacher_model, student=model)\n",
    "odd_model = distill_bert_weights_odd(teacher=teacher_model, student=model)\n",
    "even_model = distill_bert_weights_even(teacher=teacher_model, student=model)\n",
    "    \n",
    "\n",
    "\n",
    "# models = [topK, bottomK, odd_model, even_model]\n",
    "# for model in models:\n",
    "#     print(type(model))\n",
    "models = [bottomK, odd_model, even_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameters : 109484547\n",
      "Student parameters : 66957315\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Teacher parameters :', count_parameters(teacher_model))\n",
    "print('Student parameters :', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.15686353435797"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)/count_parameters(teacher_model) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It has 40% less parameters than bert-base-uncased"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "$$\n",
    "P_i(\\mathbf{z}_i, T) = \\frac{\\exp(\\mathbf{z}_i / T)}{\\sum_{q=0}^k \\exp(\\mathbf{z}_q / T)}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Distillation\n",
    "\n",
    "#### CE Loss\n",
    "$$\\mathcal{L}_\\text{CE} = -\\sum^N_{j=0}\\sum_{i=0}^k {y}_i^{(j)}\\log(P_i({v}_i^{(j)}, 1))$$\n",
    "\n",
    "#### KL Loss\n",
    "$$\\mathcal{L}_\\text{KD} = -\\sum^N_{j=0}\\sum_{i=0}^k P_i({z}_i^{(j)}, T) \\log (P_i({v}_i^{(j)}, T))$$\n",
    "\n",
    "#### Cosine Embedding Loss\n",
    "$$\\mathcal{L}_{\\text{cosine}}(x_1, x_2, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(1 - y_i \\cdot \\cos(\\theta_i)\\right)$$\n",
    "\n",
    "<!-- $$\\mathcal{L} = \\lambda \\mathcal{L}_\\text{KD} + (1-\\lambda)\\mathcal{L}_\\text{CE}$$\n",
    " -->\n",
    "\n",
    "#### Total Loss\n",
    "$$\\mathcal{L} = \\mathcal{L}_\\text{KD} + \\mathcal{L}_\\text{CE} + \\mathcal{L}_{\\text{cosine}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillKL(nn.Module):\n",
    "    \"\"\"\n",
    "    Distilling the Knowledge in a Neural Network\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistillKL, self).__init__()\n",
    "\n",
    "    def forward(self, output_student, output_teacher, temperature=1):\n",
    "        '''\n",
    "        Note: the output_student and output_teacher are logits \n",
    "        '''\n",
    "        T = temperature #.cuda()\n",
    "        \n",
    "        KD_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "            F.log_softmax(output_student/T, dim=-1),\n",
    "            F.softmax(output_teacher/T, dim=-1)\n",
    "        ) * T * T\n",
    "        \n",
    "        return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_div = DistillKL()\n",
    "criterion_cos = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 5e-5\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 2\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "# Get the metric function\n",
    "if task_name is not None:\n",
    "    metric = evaluate.load(\"glue\", task_name)\n",
    "else:\n",
    "    metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = [\"topK\", \"bottomK\", \"odd\", \"even\"]\n",
    "titles = [\"bottomK\", \"odd\", \"even\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [09:30<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottomK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 32/64 [05:18<04:07,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 1: Train loss 0.3970:\n",
      "  - Loss_cls: 1.1029\n",
      "  - Loss_div: 0.0128\n",
      "  - Loss_cos: 0.0753\n",
      "Epoch at 1: Test Acc 0.3900\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [10:45<00:00,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 2: Train loss 0.3729:\n",
      "  - Loss_cls: 1.0639\n",
      "  - Loss_div: 0.0108\n",
      "  - Loss_cos: 0.0439\n",
      "Epoch at 2: Test Acc 0.4300\n",
      "---------------------------------------------------------------------------------------\n",
      "train loss : [0.3969984846189618, 0.37286990229040384]\n",
      "train loss cls : [1.1029390692710876, 1.0639181770384312]\n",
      "train loss div : [0.012769433291396126, 0.010765262646600604]\n",
      "train loss cos : [0.0752869566786103, 0.04392627131892368]\n",
      "eval loss : [1.097914606332779, 1.0795910954475403]\n",
      "odd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [16:04,  7.88s/it]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 1: Train loss 0.3669:\n",
      "  - Loss_cls: 1.0536\n",
      "  - Loss_div: 0.0107\n",
      "  - Loss_cos: 0.0364\n",
      "Epoch at 1: Test Acc 0.4300\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [21:28,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 2: Train loss 0.3671:\n",
      "  - Loss_cls: 1.0552\n",
      "  - Loss_div: 0.0103\n",
      "  - Loss_cos: 0.0359\n",
      "Epoch at 2: Test Acc 0.4300\n",
      "---------------------------------------------------------------------------------------\n",
      "train loss : [0.36690032109618187, 0.36712944507598877]\n",
      "train loss cls : [1.0535627380013466, 1.0551958158612251]\n",
      "train loss div : [0.010706385408411734, 0.010329466487746686]\n",
      "train loss cos : [0.03643185168039054, 0.03586305532371625]\n",
      "eval loss : [1.0795910954475403, 1.0795910954475403]\n",
      "even\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [26:52,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 1: Train loss 0.3688:\n",
      "  - Loss_cls: 1.0601\n",
      "  - Loss_div: 0.0103\n",
      "  - Loss_cos: 0.0360\n",
      "Epoch at 1: Test Acc 0.4300\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [32:15,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 2: Train loss 0.3688:\n",
      "  - Loss_cls: 1.0601\n",
      "  - Loss_div: 0.0106\n",
      "  - Loss_cos: 0.0357\n",
      "Epoch at 2: Test Acc 0.4300\n",
      "---------------------------------------------------------------------------------------\n",
      "train loss : [0.36882219556719065, 0.3688040478155017]\n",
      "train loss cls : [1.0601251386106014, 1.060092743486166]\n",
      "train loss div : [0.01033403366454877, 0.010626996372593567]\n",
      "train loss cos : [0.03600742429262027, 0.035692394187208265]\n",
      "eval loss : [1.0795910954475403, 1.0795910954475403]\n",
      "Avg Metric 1.27\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "eval_metrics = 0\n",
    "\n",
    "all_train_losses = []\n",
    "all_train_losses_cls = []\n",
    "all_train_losses_div = []\n",
    "all_train_losses_cos = []\n",
    "all_eval_losses = []\n",
    "\n",
    "# Lists to store losses for each epoch\n",
    "count = 0\n",
    "for model in models:\n",
    "    save_path = f'models/{titles[count]}.pt'\n",
    "    print(titles[count])\n",
    "    count += 1\n",
    "    train_losses = []\n",
    "    train_losses_cls = []\n",
    "    train_losses_div = []\n",
    "    train_losses_cos = []\n",
    "    eval_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        teacher_model.eval()\n",
    "        train_loss = 0\n",
    "        train_loss_cls = 0\n",
    "        train_loss_div = 0\n",
    "        train_loss_cos = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # compute student output\n",
    "            outputs = model(**batch) \n",
    "            # compute teacher output\n",
    "            with torch.no_grad():\n",
    "                output_teacher = teacher_model(**batch)\n",
    "\n",
    "            # assert size\n",
    "            assert outputs.logits.size() == output_teacher.logits.size()\n",
    "            \n",
    "            # cls loss \n",
    "            loss_cls  = outputs.loss\n",
    "            train_loss_cls += loss_cls.item()\n",
    "            # distillation loss\n",
    "            loss_div = criterion_div(outputs.logits, output_teacher.logits)\n",
    "            train_loss_div += loss_div.item()\n",
    "            # cosine loss\n",
    "            loss_cos = criterion_cos(output_teacher.logits, outputs.logits, torch.ones(output_teacher.logits.size()[0]).to(device))\n",
    "            train_loss_cos += loss_cos.item()\n",
    "            \n",
    "            # Average the loss and return it\n",
    "            loss = (loss_cls + loss_div + loss_cos) / 3\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            # accelerator.backward(loss)\n",
    "            # Step with optimizer\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_losses_cls.append(train_loss_cls / len(train_dataloader))\n",
    "        train_losses_div.append(train_loss_div / len(train_dataloader))\n",
    "        train_losses_cos.append(train_loss_cos / len(train_dataloader))\n",
    "\n",
    "        print(f'Epoch at {epoch+1}: Train loss {train_loss/len(train_dataloader):.4f}:')\n",
    "        print(f'  - Loss_cls: {train_loss_cls/len(train_dataloader):.4f}')\n",
    "        print(f'  - Loss_div: {train_loss_div/len(train_dataloader):.4f}')\n",
    "        print(f'  - Loss_cos: {train_loss_cos/len(train_dataloader):.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                \n",
    "            loss_cls = outputs.loss\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            eval_loss += loss_cls.item()\n",
    "            # predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "            metric.add_batch(\n",
    "                predictions=predictions, \n",
    "                references=batch[\"labels\"])\n",
    "            \n",
    "        eval_metric = metric.compute()\n",
    "        eval_metrics += eval_metric['accuracy'] \n",
    "        eval_losses.append(eval_loss / len(eval_dataloader))  # Save the evaluation loss for plotting\n",
    "        \n",
    "        print(f\"Epoch at {epoch+1}: Test Acc {eval_metric['accuracy']:.4f}\")\n",
    "        print(\"---------------------------------------------------------------------------------------\")\n",
    "    print(f\"train loss : {train_losses}\")\n",
    "    print(f\"train loss cls : {train_losses_cls}\")\n",
    "    print(f\"train loss div : {train_losses_div}\")\n",
    "    print(f\"train loss cos : {train_losses_cos}\")\n",
    "    print(f\"eval loss : {eval_losses}\")\n",
    "    all_train_losses.append(train_loss)\n",
    "    all_train_losses_cls.append(train_loss_cls)\n",
    "    all_train_losses_div.append(train_loss_div)\n",
    "    all_train_losses_cos.append(train_loss_cos)\n",
    "    all_eval_losses.append(eval_losses)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "print('Avg Metric', eval_metrics/num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting\n",
    "epochs_list = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_list, train_losses, label='Total Train Loss')\n",
    "plt.plot(epochs_list, train_losses_cls, label='Train Loss_cls')\n",
    "plt.plot(epochs_list, train_losses_div, label='Train Loss_div')\n",
    "plt.plot(epochs_list, train_losses_cos, label='Train Loss_cos')\n",
    "plt.plot(epochs_list, eval_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Training and Validation Losses{}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix (Teacher Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [32:22, 10.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [306]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 18\u001b[0m     output_teacher \u001b[38;5;241m=\u001b[39m teacher_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# cls loss \u001b[39;00m\n\u001b[0;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m output_teacher\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:436\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:386\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 386\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    388\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "lr = 5e-5\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(params=teacher_model.parameters(), lr=lr)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "eval_metrics = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    teacher_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output_teacher = teacher_model(**batch)\n",
    "        # cls loss \n",
    "        loss = output_teacher.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "        # Step with optimizer\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f'Epoch at {epoch+1}: Train loss {train_loss/len(train_dataloader):.4f}:')\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = teacher_model(**batch)\n",
    "    \n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        # predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        metric.add_batch(\n",
    "            predictions=predictions, \n",
    "            references=batch[\"labels\"])\n",
    "        \n",
    "    eval_metric = metric.compute()\n",
    "    eval_metrics += eval_metric['accuracy'] \n",
    "    print(f\"Epoch at {epoch+1}: Test Acc {eval_metric['accuracy']:.4f}\")\n",
    "    \n",
    "print('Avg Metric', eval_metrics/num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
